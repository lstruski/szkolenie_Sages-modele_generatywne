{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie\n",
    "Proszę nauczyć trzy model \n",
    "- VAE\n",
    "- WAE\n",
    "- DCGAN\n",
    "\n",
    "Na danych Fesion MINST \n",
    "\n",
    "\n",
    "1. Maksymalnie uczymy 5 epok\n",
    "2. Liczymy FID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasa ,,*AverageMeter*'' przechowuje oraz przetwarza częściowe wyniki zapisywane w poszczegółnych etapach uczenia modelu. Funkcja ,,*count_parameters*'' zlicza liczbę parametrów sieci, zaś funkcja ,,*show*'' rysuje obrazki ze zbioru danych i ich rekonstrukcje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=2.5)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum = self.sum + val * n\n",
    "        self.count = self.count + n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def show(img, recon_img, num_col=None):\n",
    "    if recon_img is None:\n",
    "        rec_images = img\n",
    "    else:\n",
    "        n = img.shape[0]\n",
    "        assert n >= num_col\n",
    "        rec_images = torch.empty((2 * num_col, *img.shape[1:]))\n",
    "        rec_images.data[:num_col] = img.data[:num_col]\n",
    "        rec_images.data[num_col:] = recon_img.data[:num_col]\n",
    "\n",
    "    plt.figure(figsize=[16, 8])\n",
    "    grid = torchvision.utils.make_grid(\n",
    "        rec_images, nrow=num_col, padding=1, normalize=True, scale_each=True\n",
    "    )\n",
    "    np_grid = grid.cpu().numpy()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.transpose(np_grid, (1, 2, 0)), interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cpu')\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /datasets\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963425ef599c47b58dcd4f21944fbbc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26421880.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting /datasets\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to /datasets\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /datasets\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872e887e4ef54dc8a47d1fa26ff9d520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=29515.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting /datasets\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to /datasets\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /datasets\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68546e8e9664cfd8c6ca49d830816ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4422102.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting /datasets\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to /datasets\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /datasets\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ffc7d1ba534fc688b3905c92dced2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5148.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting /datasets\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to /datasets\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "root = \"/datasets\"\n",
    "download = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device=}\")\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root, download=download, train=True, transform=transform\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=False\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root, download=download, train=False, transform=transform\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=200, shuffle=False, num_workers=4, pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 3\n",
    "image, label = next(iter(train_loader))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image[index,:,:].squeeze(), cmap=\"gray\")\n",
    "print(label[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
