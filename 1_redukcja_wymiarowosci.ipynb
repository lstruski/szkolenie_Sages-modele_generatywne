{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "  Nienadzorowana reprezentacja autoenkodery i modele generatywne\n",
    "</h1>\n",
    "\n",
    "<h4 align=\"center\">\n",
    "  11.10.2023\n",
    "</h4>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redukcja wymiarowości\n",
    "\n",
    "**Analiza głównych składowych** (ang. Principal Component Analysis, PCA) jest jedną ze statystycznych metod analizy czynnikowej. Metoda ta jest często używana do zmniejszania rozmiaru zbioru danych statystycznych, poprzez odrzucenie ostatnich czynników (mniej istotnych). \n",
    "\n",
    "Głównym celem analizy PCA jest identyfikacja wzorców w danych. Celem PCA jest wykrycie korelacji między zmiennymi. W skrócie, PCA znajduje kierunki maksymalnej wariancji wysoko-wymiarowych danych, a następnie rzutuje dane na mniej-wymiarową podprzestrzeń, która zachowuje najwięcej informacji z oryginalnego zbioru. Więcej informacji na temat *PCA* można znaleźć <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">tutaj</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = np.array([[-1, -1, 4], [-2, -1, 6], [-3, -2, 2], [1, 1, -1], [2, 1, 0], [3, 2, 1]])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "    header=None, \n",
    "    sep=',')\n",
    "\n",
    "df.columns=['1', '2', '3', '4', 'class']\n",
    "df.dropna(how=\"all\", inplace=True) # usuwamy puste rekordy\n",
    "\n",
    "print(df['class'].unique())\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podział tabelki na dane i etykiety (klasy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:4].values\n",
    "y = df.iloc[:,4].values\n",
    "\n",
    "print(X[:5, :])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za pomocą histogramów możemy zobaczyć jak te trzy klasy różnią się wzdłuż każdego wymiaru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.hist(X[np.where(y == 'Iris-setosa')[0], 0], bins=7, label='Iris-setosa')\n",
    "plt.hist(X[np.where(y == 'Iris-versicolor')[0], 0], bins=7, label='Iris-versicolor')\n",
    "plt.hist(X[np.where(y == 'Iris-virginica')[0], 0], bins=7, label='Iris-virginica')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show(fig);\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.hist(X[np.where(y == 'Iris-setosa')[0], 1], bins=7, label='Iris-setosa')\n",
    "plt.hist(X[np.where(y == 'Iris-versicolor')[0], 1], bins=7, label='Iris-versicolor')\n",
    "plt.hist(X[np.where(y == 'Iris-virginica')[0], 1], bins=7, label='Iris-virginica')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show(fig);\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.hist(X[np.where(y == 'Iris-setosa')[0], 2], bins=7, label='Iris-setosa')\n",
    "plt.hist(X[np.where(y == 'Iris-versicolor')[0], 2], bins=7, label='Iris-versicolor')\n",
    "plt.hist(X[np.where(y == 'Iris-virginica')[0], 2], bins=7, label='Iris-virginica')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show(fig);\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.hist(X[np.where(y == 'Iris-setosa')[0], 3], bins=7, label='Iris-setosa')\n",
    "plt.hist(X[np.where(y == 'Iris-versicolor')[0], 3], bins=7, label='Iris-versicolor')\n",
    "plt.hist(X[np.where(y == 'Iris-virginica')[0], 3], bins=7, label='Iris-virginica')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show(fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonując analizę głównych składowych powinniśmy standaryzować dane, ponieważ PCA wybiera te podprzestrzenie (najbardziej istotne), które maksymalizują wariancję wzdłuż osi. Aby to zrobić możemy posłużyć się poniższym modułem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "print(X_std[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasyczne metody PCA obliczają wartości i wektory własne z macierzy kowariancji. Poniżej pokazujemy jak można w prosty sposób policzyć macierz kowariancji.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n",
    "# print np.cov(X_std.T) # równoważnie\n",
    "print('Macierz kowariancji: \\n%s' %cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie obliczamy problem własny dla macierzy kowariancji. Ponieważ jest to macierz symetryczna dlatego też użyjemy metody, która oblicza problem własny dla macierzy symetrycznej (jest szybsza, mniej złożona obliczeniowo niż metoda licząca problem własny dla macierzy niesymetrycznych)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eigh(cov_mat)\n",
    "\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czasem zamiast macierzy kowariancji oblicza się macierz korelacji (szczególnie w finansach). Warto zauważyć, że problem własny dla macierzy kowariancji (jeżeli dane wejściowe są standaryzowane) daje takie same wyniki jak dla macierzy korelacji (macierz korelacji może być rozumiana jako znormalizowana macierzy kowariancji)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(X_std.T)\n",
    "print(corr, \"\\n\")\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eigh(corr)\n",
    "\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr1 = np.corrcoef(X.T)\n",
    "print(corr1, \"\\n\")\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eigh(corr1)\n",
    "\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że wszystkie trzy podejścia dają te same wektory i wartości własnej. Większość metod PCA (w implementacji) wykorzystuje rozkład SVD (<a href=\"https://pl.wikipedia.org/wiki/Rozk%C5%82ad_wed%C5%82ug_warto%C5%9Bci_osobliwych\">zobacz</a>) w celu poprawy wydajności obliczeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,v = np.linalg.svd(X_std.T)\n",
    "print(u, '\\n')\n",
    "print(s, '\\n')\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdy już mamy wartości i wektory własne, należy je posortować. Sortujemy malejąco, ponieważ będziemy rzutować na podprzestrzenie (opisane przez wektory własne) przechowujące więcej informacji ze zbioru danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# czy wektory są znormalizowane\n",
    "for ev in eig_vecs:\n",
    "    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "print('Ok!')\n",
    "\n",
    "# lista krotek (eigenvalue, eigenvector)\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) \n",
    "             for i in range(len(eig_vals))]\n",
    "\n",
    "# Sortowanie (eigenvalue, eigenvector)\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy już zmniejszyć wymiar 4-wymiarowych danych do 2-wymiarowe dane. W tym celu wybieramy dwa wektory własne odpowiadające dwóm największym wartością własnym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.hstack((eig_pairs[0][1].reshape(-1,1), eig_pairs[1][1].reshape(-1,1)))\n",
    "print('Macierz W:\\n', W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X_std.dot(W)\n",
    "Y[:, 0] = Y[:, 0] * -1\n",
    "\n",
    "print(X_std[:6,:], '\\n')\n",
    "print(Y[:6, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['b', 'c', 'r']\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(Y[np.where(y == 'Iris-setosa')[0], 0], Y[np.where(y == 'Iris-setosa')[0], 1],\n",
    "            marker='x', color=colors[0], label='Iris-setosa')\n",
    "plt.scatter(Y[np.where(y == 'Iris-versicolor')[0], 0], Y[np.where(y == 'Iris-versicolor')[0], 1], \n",
    "            marker='^', color=colors[1], label='Iris-versicolor')\n",
    "plt.scatter(Y[np.where(y == 'Iris-virginica')[0], 0], Y[np.where(y == 'Iris-virginica')[0], 1], \n",
    "            marker='o', color=colors[2], label='Iris-virginica')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show(fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteka <a href=\"http://scikit-learn.org/stable/index.html\">**sklearn**</a> dostarcza nam funkcję <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">*PCA*</a>, która wykonuje wszystkie powyższe operacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "\n",
    "\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "Y_sklearn = sklearn_pca.fit_transform(X_std)\n",
    "print(Y_sklearn[:6, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['b', 'c', 'r']\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(Y_sklearn[np.where(y == 'Iris-setosa')[0], 0], Y_sklearn[np.where(y == 'Iris-setosa')[0], 1],\n",
    "            marker='x', color=colors[0], label='Iris-setosa')\n",
    "plt.scatter(Y_sklearn[np.where(y == 'Iris-versicolor')[0], 0], Y_sklearn[np.where(y == 'Iris-versicolor')[0], 1], \n",
    "            marker='^', color=colors[1], label='Iris-versicolor')\n",
    "plt.scatter(Y_sklearn[np.where(y == 'Iris-virginica')[0], 0], Y_sklearn[np.where(y == 'Iris-virginica')[0], 1], \n",
    "            marker='o', color=colors[2], label='Iris-virginica')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show(fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "Wylosuj po 10 cyfr ze zbioru cyfr i wykonaj na nim redukcję wymiarowość do 2. Narysuj położenie tych punktów tak jak zrobiliśmy to dla zbioru Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
